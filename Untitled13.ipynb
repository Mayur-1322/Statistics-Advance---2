{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment no - 3 ( Statistics Advance - 2 )**"
      ],
      "metadata": {
        "id": "y1IEqsyjzCfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1.Shape and Parameters:\n",
        "The F-distribution is skewed to the right (positively skewed), meaning its tail extends towards the right.\n",
        "It is non-negative; that is, its values are always greater than or equal to 0.\n",
        "2. Probability Density Function (PDF):\n",
        "The probability density function for the F-distribution with\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "df\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "df\n",
        "2\n",
        "​\n",
        "  degrees of freedom is given by:\n",
        "\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ";\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        ",\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        ")\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "2\n",
        "𝑥\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "2\n",
        "−\n",
        "1\n",
        "𝐵\n",
        "(\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "2\n",
        ",\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "2\n",
        ")\n",
        "(\n",
        "1\n",
        "+\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "𝑥\n",
        ")\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "+\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "2\n",
        "f(x;df\n",
        "1\n",
        "​\n",
        " ,df\n",
        "2\n",
        "​\n",
        " )=\n",
        "B(\n",
        "2\n",
        "df\n",
        "1\n",
        "​\n",
        "\n",
        "​\n",
        " ,\n",
        "2\n",
        "df\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        " )(1+\n",
        "df\n",
        "2\n",
        "​\n",
        "\n",
        "df\n",
        "1\n",
        "​\n",
        "\n",
        "​\n",
        " x)\n",
        "2\n",
        "df\n",
        "1\n",
        "​\n",
        " +df\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "\n",
        "(\n",
        "df\n",
        "2\n",
        "​\n",
        "\n",
        "df\n",
        "1\n",
        "​\n",
        "\n",
        "​\n",
        " )\n",
        "2\n",
        "df\n",
        "1\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        " x\n",
        "2\n",
        "df\n",
        "1\n",
        "​\n",
        "\n",
        "​\n",
        " −1\n",
        "\n",
        "​\n",
        "\n",
        "where\n",
        "𝑥\n",
        "≥\n",
        "0\n",
        "x≥0, and\n",
        "𝐵\n",
        "(\n",
        "⋅\n",
        ",\n",
        "⋅\n",
        ")\n",
        "B(⋅,⋅) is the Beta function.\n",
        "\n",
        "3. Mean:\n",
        "The mean of the F-distribution depends on the degrees of freedom of the numerator and denominator:\n",
        "\n",
        "For\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        ">\n",
        "2\n",
        "df\n",
        "2\n",
        "​\n",
        " >2, the mean is given by:\n",
        "𝜇\n",
        "=\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "−\n",
        "2\n",
        "μ=\n",
        "df\n",
        "2\n",
        "​\n",
        " −2\n",
        "df\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "The mean is only defined when\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        ">\n",
        "2\n",
        "df\n",
        "2\n",
        "​\n",
        " >2. If\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "≤\n",
        "2\n",
        "df\n",
        "2\n",
        "​\n",
        " ≤2, the mean does not exist.\n",
        "4. Variance:\n",
        "The variance of the F-distribution also depends on the degrees of freedom:\n",
        "\n",
        "For\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        ">\n",
        "4\n",
        "df\n",
        "2\n",
        "​\n",
        " >4, the variance is given by:\n",
        "𝜎\n",
        "2\n",
        "=\n",
        "2\n",
        "(\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "2\n",
        "+\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "−\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        ")\n",
        "𝑑\n",
        "𝑓\n",
        "1\n",
        "(\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "−\n",
        "2\n",
        ")\n",
        "2\n",
        "(\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        "−\n",
        "4\n",
        ")\n",
        "σ\n",
        "2\n",
        " =\n",
        "df\n",
        "1\n",
        "​\n",
        " (df\n",
        "2\n",
        "​\n",
        " −2)\n",
        "2\n",
        " (df\n",
        "2\n",
        "​\n",
        " −4)\n",
        "2(df\n",
        "2\n",
        "2\n",
        "​\n",
        " +df\n",
        "1\n",
        "​\n",
        " −df\n",
        "2\n",
        "​\n",
        " )\n",
        "​\n",
        "\n",
        "The variance is defined only when\n",
        "𝑑\n",
        "𝑓\n",
        "2\n",
        ">\n",
        "4\n",
        "df\n",
        "2\n",
        "​\n",
        " >4.\n",
        "5. Skewness:\n",
        "The F-distribution is positively skewed (right-skewed). The degree of skewness decreases as the degrees of freedom of the numerator and denominator increase.\n",
        "6. Kurtosis:\n",
        "The F-distribution has a leptokurtic shape, meaning that it has heavier tails compared to a normal distribution, especially for smaller degrees of freedom. The kurtosis depends on the degrees of freedom."
      ],
      "metadata": {
        "id": "k6OUeaKVzXh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
      ],
      "metadata": {
        "id": "VOx7keyszq2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Analysis of Variance (ANOVA)\n",
        "Purpose: ANOVA is used to test whether there are significant differences between the means of three or more groups.\n",
        "Why F-distribution is used:\n",
        "In ANOVA, the ratio of between-group variance to within-group variance is tested. The F-statistic is used to determine if the variation between group means is larger than what would be expected by random chance.\n",
        "Under the null hypothesis (no difference in means between groups), the expected value of the F-statistic follows an F-distribution with degrees of freedom corresponding to the number of groups and the sample sizes within those groups.\n",
        "\n",
        "2. F-test for Comparing Two Variances\n",
        "Purpose: This test is used to determine whether two populations have the same variance. It compares the ratio of the variances of two independent samples from normal distributions.\n",
        "Why F-distribution is used:\n",
        "The F-statistic is the ratio of two sample variances, each divided by their respective degrees of freedom. If the populations have equal variances, the ratio of these variances follows an F-distribution.\n",
        "This test assumes that both populations are normally distributed, and the F-distribution is the natural distribution for the ratio of two independent chi-square variables (which are often used to estimate variances).\n",
        "3. Regression Analysis: F-test for Overall Model Significance\n",
        "Purpose: In regression analysis, the F-test is used to assess the overall significance of the regression model, i.e., whether the independent variables, collectively, explain a significant amount of variation in the dependent variable.\n",
        "Why F-distribution is used:\n",
        "In multiple regression, the F-statistic compares the model with the explanatory variables (predictors) to a model with no predictors (the null model). It tests if the inclusion of the predictors significantly improves the model's fit.\n",
        "The F-statistic is calculated as the ratio of the explained variance to the unexplained variance. Under the null hypothesis (no relationship between the predictors and the response variable), this ratio follows an F-distribution.\n",
        "\n",
        "4. Nested Models and Model Comparison\n",
        "Purpose: The F-test is used to compare nested models (models where one is a special case of the other). This test is common in both linear regression and generalized linear models when testing if adding more predictors significantly improves the model fit.\n",
        "Why F-distribution is used:\n",
        "When comparing nested models, the F-statistic is used to test if the more complex model (with additional parameters) explains significantly more variance than the simpler model.\n",
        "The F-statistic compares the improvement in model fit (through a change in the residual sum of squares) to the increase in model complexity (additional parameters).\n",
        "5. Levene's Test for Homogeneity of Variances\n",
        "Purpose: Levene’s test is used to assess whether different groups have equal variances, which is an important assumption for many parametric tests, including ANOVA and t-tests.\n",
        "Why F-distribution is used:\n",
        "Levene’s test uses the F-statistic to compare the variances of groups after transforming the data (usually by taking the absolute deviations from the group medians or means). The F-distribution is used because the test statistic is based on the ratio of variances from different group.\n",
        "\n",
        "Why is the F-distribution Appropriate for These Tests?\n",
        "The F-distribution is appropriate for these tests primarily because it arises from comparing variances or variability in different data sets or models. Here’s why it is a natural choice:\n",
        "\n",
        "Ratio of Variances: In each of these tests (e.g., ANOVA, F-test for variances, regression F-test), the test statistic involves the ratio of two variances (e.g., between-group variance vs. within-group variance, or explained variance vs. residual variance). The ratio of two independent chi-square variables follows an F-distribution.\n",
        "\n",
        "Asymmetry and Skewness: The F-distribution is positively skewed (right-tailed), which reflects the nature of variance ratios — variances cannot be negative, and large ratios are possible but increasingly unlikely as the degrees of freedom in increse\n",
        "\n"
      ],
      "metadata": {
        "id": "dWwg8S_wztEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SG1JkUu_8BgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Independence of Samples\n",
        "Assumption: The two samples must be independent of each other.\n",
        "Explanation: Each sample should be drawn from a different population (or group), and the selection of one sample should not affect the selection of the other sample. This is important because the F-test compares the variability between two independent groups.\n",
        "2. Normality of the Population\n",
        "Assumption: The populations from which the samples are drawn should follow a normal distribution (or at least approximate normality).\n",
        "Explanation: The F-test assumes that the data in both populations are approximately normally distributed because the variances are estimated from sample data. If the populations are not normal, the F-test may produce misleading results, especially if the sample sizes are small.\n",
        "Note: The assumption of normality is more important for small sample sizes.\n",
        "3. Homogeneity of Variances (or Equal Variances)\n",
        "Assumption: The variances of the two populations should be equal under the null hypothesis.\n",
        "Explanation: The F-test compares the ratio of two sample variances (one from each group). If the null hypothesis is true, the two population variances are assumed to be equal, and the ratio of the sample variances should follow an F-distribution. If the null hypothesis is rejected, it indicates that the variances are significantly different.\n",
        "Note: The F-test is specifically designed to test this assumption of equal variances.\n",
        "4. Independent sampling from each population\n",
        "Assumption: Each of the two samples must be selected independently from its respective population.\n",
        "Explanation: This assumption is critical because the F-test compares variances from two separate populations. If the observations within each sample are not independent, the variability within each group may be underestimated or overestimated, which can distort the F-statistic.\n",
        "5. Continuous Data\n",
        "Assumption: The data should be continuous (or approximately continuous).\n",
        "Explanation: The F-test assumes that the data are measured on an interval or ratio scale (i.e., continuous), as it relies on the calculation of variances, which are based on the squared differences between data points."
      ],
      "metadata": {
        "id": "9c_kFbHy8C3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the purpose of ANOVA, and how does it differ from a t-test?"
      ],
      "metadata": {
        "id": "uKWYjjTh1Sxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose of ANOVA **(Analysis of Variance):\n",
        "ANOVA is a statistical technique used to compare the means of three or more groups to determine if there are any statistically significant differences between them. Specifically, ANOVA assesses whether the variability within each group is smaller than the variability between the groups, which would indicate that the group means are not all the same.\n",
        "\n",
        "Key Purposes of ANOVA:\n",
        "Testing for Differences in Means: The primary purpose of ANOVA is to test the null hypothesis that all group means are equal. It checks if there is enough evidence to reject this hypothesis and conclude that at least one group mean is different from the others.\n",
        "Partitioning Variance: ANOVA divides the total variance observed in the data into two components: the variance between groups and the variance within groups."
      ],
      "metadata": {
        "id": "wL4s_pXf1Tkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How ANOVA Differs from a t-test:\n",
        "The t-test and ANOVA are both used to compare means, but they have different purposes, assumptions, and appropriate applications. Here’s a comparison:\n",
        "\n",
        "1. Number of Groups Being Compared:\n",
        "t-test: Typically used to compare two groups (i.e., testing the difference between the means of two independent or paired groups).\n",
        "Independent t-test: Compares the means of two independent groups.\n",
        "Paired t-test: Compares the means of two related groups (e.g., before-and-after measurements on the same subjects).\n",
        "ANOVA: Used when you have three or more groups to compare. It tests whether there are significant differences between the means of multiple groups simultaneously.\n",
        "2. Hypotheses Tested:\n",
        "t-test: Tests the difference between the means of two groups.\n",
        "Null hypothesis:\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜇\n",
        "1\n",
        "=\n",
        "𝜇\n",
        "2\n",
        "H\n",
        "0\n",
        "​\n",
        " :μ\n",
        "1\n",
        "​\n",
        " =μ\n",
        "2\n",
        "​\n",
        "  (no difference between the two means).\n",
        "ANOVA: Tests whether there is a significant difference in means among more than two groups.\n",
        "Null hypothesis:\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜇\n",
        "1\n",
        "=\n",
        "𝜇\n",
        "2\n",
        "=\n",
        "⋯\n",
        "=\n",
        "𝜇\n",
        "𝑘\n",
        "H\n",
        "0\n",
        "​\n",
        " :μ\n",
        "1\n",
        "​\n",
        " =μ\n",
        "2\n",
        "​\n",
        " =⋯=μ\n",
        "k\n",
        "​\n",
        "  (all group means are equal).\n",
        "3. Test Statistic:\n",
        "t-test: The test statistic is the t-statistic, calculated by comparing the difference between the sample means to the standard error of the difference.\n",
        "ANOVA: The test statistic is the F-statistic, which is the ratio of the variance between groups to the variance within groups.\n",
        "4. Multiple Comparisons:\n",
        "t-test: The t-test is limited to comparing only two groups. If you need to compare more than two groups, you would have to conduct multiple t-tests. However, performing multiple t-tests increases the risk of a Type I error (false positives), as the probability of incorrectly rejecting the null hypothesis increases with each additional test.\n",
        "ANOVA: ANOVA allows for simultaneous comparison of multiple groups without increasing the risk of Type I errors."
      ],
      "metadata": {
        "id": "WRTyjnef1qXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups."
      ],
      "metadata": {
        "id": "IPJh6iEk1xUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Controls the Type I Error Rate: Each time a t-test is performed, there is a chance of making a Type I error (incorrectly rejecting the null hypothesis). If you perform multiple t-tests, the probability of committing a Type I error increases with each additional test. A one-way ANOVA, however, compares all group means simultaneously in a single test, keeping the overall error rate at the desired significance level (e.g., 0.05).\n",
        "\n",
        "2.Efficiency and Simplicity: Running a single ANOVA test to determine if there are any statistically significant differences among group means is more efficient than running multiple t-tests. With\n",
        "𝑘\n",
        "k groups, you'd need\n",
        "𝑘\n",
        "(\n",
        "𝑘\n",
        "−\n",
        "1\n",
        ")\n",
        "/\n",
        "2\n",
        "k(k−1)/2 t-tests, which can be cumbersome and time-consuming with many groups. One-way ANOVA streamlines this by providing a single test result.\n",
        "\n",
        "3.Interpretation of Results: A one-way ANOVA can indicate whether at least one group mean is different from the others, which is the primary question when comparing multiple groups. If ANOVA shows significance, post-hoc tests can then be used to explore specific group differences if needed, without increasing the risk of Type I error as much."
      ],
      "metadata": {
        "id": "JU4WQL8A1yCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?"
      ],
      "metadata": {
        "id": "VYRr2vWa2Wbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Total Variance\n",
        "The total variance represents all the variability in the dataset and is based on the differences between each individual data point and the overall mean of all data points (grand mean).\n",
        "\n",
        "2. Between-Group Variance (Explained Variance)\n",
        "Between-group variance, also called explained variance, reflects the variability due to differences between the group means. It is calculated by examining how much each group mean deviates from the overall (grand) mean. This component of variance captures the variability attributable to the treatment or condition being tested.\n",
        "\n",
        "Calculation: For each group, the difference between the group mean and the grand mean is squared, multiplied by the number of data points in that group, and summed across all groups.\n",
        "Purpose: If this between-group variance is large relative to within-group variance, it suggests that at least one group mean is significantly different from the others, indicating a potential treatment effect.\n",
        "3. Within-Group Variance (Unexplained Variance)\n",
        "Within-group variance, also known as error variance or residual variance, represents the variability within each group. It reflects individual differences or random error and is calculated by examining how much each data point in a group deviates from its respective group mean.\n",
        "\n",
        "Calculation: For each data point, the difference between the individual value and its group mean is squared, and these values are summed across all groups.\n",
        "Purpose: This within-group variance represents the \"noise\" in the data that is not explained by the group differences.\n",
        "4. Calculation of the F-Statistic\n",
        "The F-statistic in a one-way ANOVA is a ratio that compares the between-group variance to the within-group variance:\n",
        "\n",
        "𝐹\n",
        "=\n",
        "Between-group variance (Mean Square Between)\n",
        "Within-group variance (Mean Square Within)\n",
        "F=\n",
        "Within-group variance (Mean Square Within)\n",
        "Between-group variance (Mean Square Between)\n",
        "​\n",
        "\n",
        "1.Mean Square Between (MSB): The between-group variance is divided by its degrees of freedom (number of groups - 1) to get the mean square between.\n",
        "2.Mean Square Within (MSW): The within-group variance is divided by its degrees of freedom (total sample size - number of groups) to get the mean square within.\n",
        "\n",
        "If the F-statistic is significantly larger than 1, it suggests that the between-group variance is greater than the within-group variance, meaning that the group means are not all equal."
      ],
      "metadata": {
        "id": "pY3Z12Lu2XO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
      ],
      "metadata": {
        "id": "6sdCNDsD2y0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Uncertainty\n",
        "Frequentist ANOVA: In the frequentist approach, uncertainty is typically expressed in terms of p-values and confidence intervals. A p-value indicates the probability of observing data as extreme as the actual data, given that the null hypothesis (no difference among groups) is true.\n",
        "Bayesian ANOVA: Bayesian ANOVA models uncertainty through probability distributions over parameters, providing a more direct expression of uncertainty.\n",
        "\n",
        "2. Parameter Estimation\n",
        "Frequentist ANOVA: The frequentist approach estimates parameters (such as group means and variances) as fixed values based on the observed data, assuming they are single points with no associated distribution. Parameters are not assumed to vary and are simply calculated from sample statistics.\n",
        "Bayesian ANOVA: In the Bayesian approach, parameters are treated as random variables with distributions that represent our uncertainty. Prior distributions are assigned to parameters based on prior knowledge\n",
        "\n",
        "\n",
        "3. Hypothesis Testing\n",
        "Frequentist ANOVA: Hypothesis testing in frequentist ANOVA involves setting up a null hypothesis (e.g., all group means are equal) and using the F-statistic and corresponding p-value to decide whether to reject the null hypothesis.\n",
        "Bayesian ANOVA: In Bayesian ANOVA, hypothesis testing is often framed as model comparison. For example, rather than testing if all group means are equal, Bayesian ANOVA might compare the probability of a model with equal means across groups to a model with differing means. This is done using metrics like Bayes factors, which provide the ratio of evidence for one model over another, allowing for statements like \"the data are 5 times more likely under Model A than Model B.\""
      ],
      "metadata": {
        "id": "o3h5naIt2z2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "V Profession A: [48, 52, 55, 60, 62'\n",
        "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison"
      ],
      "metadata": {
        "id": "fWg_-XWb8rO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-by-step process:**\n",
        "\n",
        "**Calculate the sample variances** for each profession's income.\n",
        "\n",
        "**Compute the F-statistic**, which is the ratio of the larger variance to the smaller variance.\n",
        "\n",
        "**Determine the p-value** for the F-statistic using the F-distribution with appropriate degrees of freedom.\n",
        "\n",
        "**Formula for F-statistic**:\n",
        "[ F = \\frac{\\text{larger variance}}{\\text{smaller variance}} ] where:\n",
        "\n",
        "**larger variance** is the variance of the profession with the higher variance.\n",
        "\n",
        "**smaller variance**is the variance of the profession with the lower variance.\n",
        "\n",
        "**Explanation of the Code:**\n",
        "**Data**: The two sets of income data for Profession A and Profession B.\n",
        "\n",
        "**Variance Calculation**: We calculate the sample variance using np.var() with ddof=1 (for sample variance).\n",
        "\n",
        "**F-statistic**: We compute the ratio of the larger variance to the smaller variance.\n",
        "\n",
        "**p-value**: Using scipy.stats.f.cdf(), we find the cumulative distribution function (CDF) for the F-distribution and calculate the p-value corresponding to the computed F-statistic.\n",
        "\n",
        "**Conclusion:** Based on the p-value, we decide whether to reject or fail to reject the null hypothesis. The null hypothesis for the F-test is that the variances of the two populations are equal.\n",
        "\n",
        "**Interpretation:**\n",
        "The F-statistic is calculated as approximately 4.16.\n",
        "\n",
        "The p-value is approximately 0.038, which is less than the typical significance level of 0.05.\n",
        "\n",
        "Since the p-value is smaller than 0.05, we reject the null hypothesis and conclude that the variances of the incomes of Profession A and Profession B are significantly different."
      ],
      "metadata": {
        "id": "6YkfSB2r8sMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "var_a = np.var(profession_a, ddof=1)\n",
        "var_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "if var_a > var_b:\n",
        "    f_statistic = var_a / var_b\n",
        "    df1 = len(profession_a) - 1\n",
        "    df2 = len(profession_b) - 1\n",
        "else:\n",
        "    f_statistic = var_b / var_a\n",
        "    df1 = len(profession_b) - 1\n",
        "    df2 = len(profession_a) - 1\n",
        "\n",
        "p_value = 1 - f.cdf(f_statistic, df1, df2)\n",
        "\n",
        "print(f\"Variance of Profession A: {var_a}\")\n",
        "print(f\"Variance of Profession B: {var_b}\")\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"Degrees of freedom: df1 = {df1}, df2 = {df2}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: Reject the null hypothesis. The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. The variances are not significantly different.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YJnKkws8y9G",
        "outputId": "b7604365-999e-4f93-c4db-32ae2dc340f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of Profession A: 32.8\n",
            "Variance of Profession B: 15.7\n",
            "F-statistic: 2.089171974522293\n",
            "Degrees of freedom: df1 = 4, df2 = 4\n",
            "p-value: 0.24652429950266952\n",
            "Conclusion: Fail to reject the null hypothesis. The variances are not significantly different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        "V Region A: [160, 162, 165, 158, 164'\n",
        "V Region B: [172, 175, 170, 168, 174'\n",
        "V Region C: [180, 182, 179, 185, 183'\n",
        "V Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "JCQGUtpv96kF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps**:\n",
        "**1.State the hypotheses**:\n",
        "\n",
        "**Null Hypothesis (H₀):** The means of the three regions are equal.\n",
        "\n",
        "**Alternative Hypothesis (H₁)**: At least one region mean is different from the others.\n",
        "\n",
        "**Calculate the F-statistic:**\n",
        "\n",
        "One-way ANOVA compares the variance between the groups (regions) to the variance within the groups. The F-statistic is the ratio of the between-group variance to the within-group variance.\n",
        "\n",
        "**Interpret the p-value:**\n",
        "\n",
        "A **low p-value **(typically < 0.05) indicates that at least one group mean is significantly different from the others.\n",
        "\n",
        "A** high p-value **(typically > 0.05) suggests that there is no significant difference between the group means.\n",
        "\n",
        "**Explanation of the Code:**\n",
        "\n",
        "**Data:** We define the height data for three regions: region_a, region_b, and region_c.\n",
        "\n",
        "**ANOVA Calculation:** We use scipy.stats.f_oneway() to perform the one-way ANOVA. This function computes the F-statistic and the associated p-value by comparing the group means.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "1.We compare the p-value with the significance level (alpha = 0.05).\n",
        "\n",
        "2.If the p-value is less than alpha, we reject the null hypothesis, indicating that there is a significant difference in average heights between the regions.\n",
        "\n",
        "3.If the p-value is greater than or equal to alpha, we fail to reject the null hypothesis, indicating that there is no significant difference between the group means.\n",
        "\n",
        "**Interpretation:**\n",
        "1.The F-statistic is approximately **43.98**, which is quite large.\n",
        "\n",
        "2.The **p-value** is approximately 0.**0000129**, which is much smaller than the significance level **(α = 0.05).**\n",
        "\n",
        "3.Since the p-value is less than 0.05, we **reject the null hypothesis,** concluding that there is a statistically significant difference in average heights between the three regions.\n",
        "\n",
        "**Conclusion:**\n",
        "Based on the results of the one-way ANOVA, we can conclude that the average heights are significantly different between the three regions (Region A, Region B, and Region C)."
      ],
      "metadata": {
        "id": "c_JDbVI897Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: Reject the null hypothesis. There is a significant difference in average heights between the regions.\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. There is no significant difference in average heights between the regions.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qeFSe-FBn29",
        "outputId": "7376a7db-cd98-4f28-f317-99bba098bdc9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "Conclusion: Reject the null hypothesis. There is a significant difference in average heights between the regions.\n"
          ]
        }
      ]
    }
  ]
}